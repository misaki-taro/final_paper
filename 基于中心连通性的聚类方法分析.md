# 基于中心连通性的聚类方法分析

## 1. 绪论

​		聚类分析起源于分类学，在古老的分类学中，人们主要依靠经验和专业知识来实现分类，很少利用数学工具进行定量的分类,致使许多分类带有主观性和任意性，不能很好地揭示客观事物内在的本质区别与联系，特别是对于多个特征、多指标的分类问题更难以实现客观的分类。随着人类科学技术的发展，对分类的要求越来越高，于是人们逐渐地把数学工具引用到了分类学中，形成了数值分类学，之后又将多元分析的技术引入到数值分类学形成了聚类分析。

​		聚类分析在许多领域中都得到了广泛的应用，取得了许多成果。

### 1.1 聚类分析的基本思想

​		聚类分析认为，所研究的样本之间存在不同程度的相似性。根据多个样本的多个特征，找出能够表示样本之间的相似性的度量，并且根据这种度量采用某些聚类方法，将所有的样品分配到不同的种类当中去，使同一种类中的样本具有较大的相似性，不同类中的样本相似性小。我们将这种分类方法称为聚类分析。

### 1.2 聚类分析的内容

​		聚类中心和聚类数目的确定是聚类分析的关键。许多聚类方法已经被广泛探索过。常见的聚类方法有K-Means聚类、均值漂移聚类、基于密度的聚类、层次聚类等聚类方法、基于图论的聚类等聚类方法。本文主要介绍基于中心连通性的聚类方法。基于中心连通性的聚类方法是基于图论的方法。

​		每种聚类分析方法都涉及事物之间的相似性。聚类分析方法的本质就是寻找一个客观能反应样本之间相关联系的统计量，然后根据这种统计量把样本分成若干类，常用的统计量有距离和相似系数。

#### 1.2.1相似性度量——距离

用距离来衡量样本之间的相似程度。假设$$d(x_i,x_j)$$是样本$$x_i$$和样本$$x_j$$的距离。常用距离:

欧式距离: $$d(x_i,x_j)=[\sum_{k=1}^{p}(x_{ik}-x_{jk})^2]^\frac{1}{2}$$;

绝对距离: $$d(x_i,x_j)=\sum_{k=1}^{p}\left | x_{ik}-x_{jk} \right |$$;

Minkowski距离: $$d(x_i,x_j)=[\sum_{k=1}^{p}(x_{ik}-x_{jk})^m]^\frac{1}{m}$$



#### 1.2.2相似性度量——相关系数

对n个样本进行聚类的时候，用相似系数来衡量变量之间相关联程度。用$$c_{\alpha \beta }$$表示样本$$x_{\alpha}$$和样本$$x_{\beta}$$之间的相似系数，应当满足以下条件：

(1)	$$\left | c_{\alpha \beta } \right |\leqslant 1$$且$$c_{\alpha \alpha }= 1$$

(2)	$$c_{\alpha \beta }=_{\beta \alpha }$$

$$\left | c_{\alpha \beta } \right |$$越接近1，说明$$x_{\alpha}$$和$$x_{\beta}$$越相关，相似系数中最常用的是相关系数和夹角余弦。在基于图的聚类中，我们通常使用高斯核函数。鉴于本文研究内容，下面介绍基于中心连通性的聚类方法相关内容。



### 1.3常见的聚类算法

1.3.1 K-Means聚类算法

​		给定样本集$$D=\{ x_1,x_2,...,x_m\}$$，按照样本之间的距离大小，划分成$$k$$个簇$$C=\{C_1,C_2,...,C_k\}$$，经过$$N$$次迭代，让簇内的点紧密联系，而簇间的距离尽可能大。

​		**步骤1**	从样本集$$D$$中随机选取$$k$$个样本作为初始的$$k$$个质心向量$$\{\mu_1,\mu_2,...,\mu_k\}$$

​		**步骤2**	对于每一次迭代$$n=1,2,...,N$$,首先将簇$$C$$初始化为$$C_t=\emptyset,t=1,2,...,k$$。对于样本集$$D$$中的各样本点$$x_i(i=1,2,...,m)$$,计算他们与各质心向量$$\mu_j(j=1,2,...,k)$$的距离$$d_{ij}=\|x_i-\mu_j\|_{2}^{2}$$。将$$x_i$$标记最小的$$d_{ij}$$所对应的类别$$\lambda_i$$,更新$$C_{\lambda i}=C_{\lambda i} \cup \{x_i \}$$。对于$$j=1,2,...,k$$，对$$C_j$$中所有的样本点重新计算质心$$\mu_j=\frac {1} {\left | C_j \right |}\sum\limits_{x \in C_j}x$$,如果所有的k个质心向量都没有发生变化，则输出簇划分$$C=\{ C_1,C_2,...,C_k\}$$，否则继续迭代。

​		K-Means聚类算法的优点是速度快，计算简便，但是必须要提前将数据划分为指定的类别数，而且聚类结果受初始选择点的影响。

1.3.2DBSCAN

​		DBSCAN(具有噪声的基于密度的聚类方法)是一种基于密度的空间聚类算法。这类密度算法通过样本分布的紧密程度进行分类，即将具有足够密度的区域划分为簇，并在具有噪声的空间数据样本中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。

​		**相关定义**	首先将给定样本集$$D=\{ x_1,x_2,...,x_m\}$$通过半径$$Eps$$和样本个数阈值$$MinPts$$将数据分为三类

​		核心点：样本$$x_i$$的半径$$Eps$$的邻域内至少包含$$MinPts$$个样本，则称$$x_i$$为核心点。

​		边界点：样本$$x_i$$的半径$$Eps$$的邻域内包含的样本点数目小于$$MinPts$$，但是它在其他核心点的邻域内，则称样本$$x_i$$为边界点。

​		噪声点：样本$$x_i$$的半径$$Eps$$的邻域内包含的样本点数目小于$$MinPts$$，同时$$x_i$$也不在其他核心点的邻域内，则称样本$$x_i$$为噪声点。

​		**步骤1**	给定样本集$$D=\{ x_1,x_2,...,x_m\}$$,邻域参数($$Eps$$,$$MinPts$$),初始化核心点集合$$\Omega=\emptyset$$,初始化聚类簇数$$k=0$$,初始化未访问样本集合$$\Gamma=D$$,簇划分$$C=\emptyset$$。

​		**步骤2**	对于$$j=1,2,...,m$$，寻找核心点。根据核心点的定义，如果$$x_j$$是核心点，则将$$x_j$$加入核心点集合$$\Omega = \Omega \cup \{ x_j\}$$.

​		**步骤3**	如果核心点集合$$\Omega = \emptyset$$,则算法结束，否则进入步骤4。

​		**步骤4**	在核心点集合$$\Omega$$中，选择一个核心点$$o$$,初始化当前簇核心点集合$$\Omega _{cur}=\{ o\}$$,初始化簇类序号$$k=k+1$$,初始化当前样本集合$$C_k=\{o\}$$,更新未访问样本集合$$\Gamma = \Gamma-\{ o\}$$.

​		**步骤5**	如果当前簇核心点集合$$\Omega _{cur}=\emptyset$$,则当前聚类簇$$C_k$$生成完毕，更新簇划分$$C=\{C_1,C_2,...,C_k\}$$,更新核心点集合$$\Omega=\Omega-C_k$$,转入步骤3，否则到步骤6

​		**步骤6**	在当前簇核心点集合$$\Omega_{cur}$$中取出一个核心点$$o'$$,通过$$Eps$$找出邻域内所有样本集$$N(o')$$,令$$\Delta = N(o') \cap \Gamma$$,更新当前簇样本集合$$C_k = C_k \cup \Delta$$,更新未访问样本集合$$\Gamma = \Gamma-\Delta$$,更新$$\Omega=\Omega \cup (\Delta \cap \Omega)-o'$$,转入步骤5

​		最终输出簇划分结果$$C=\{C_1,C_2,...,C_k \}$$

​		DBSCAN的优点是不需要预先声明需要划分簇类的数目$$k$$,并且可以对任意形状稠密的数据集进行分类；在聚类的同时，可以发现异常点（噪声点），对数据集的异常点(噪声点)不敏感；不仅如此，相较于K-Means聚类算法，DBSCAN不受初始值选取的影响。

​		然而，当空间聚类的密度不均匀，参数$$Eps$$和$$MinPts$$选取困难；当数据集较大的时候，聚类收敛的时间会相对较长；相对于传统的K-Means聚类算法稍显复杂，因为需要调整参数$$Eps$$和$$MinPts$$,不同的参数组合对实际的聚类结果影响较大。



## 2.基于中心连通性的聚类方法及其研究

### 2.1基于中心连通性的聚类方法

​		基于中心连通性的聚类方法是基于图论游走次数理论的。唯一不同的是，在基于中心连通性的聚类方法当中，在无向图中的每一个顶点都是自连的。

​		我们认为聚类是一种演化的、动态的形式。一个数据点是否是聚类中心取决于我们怎样去观察它。我们将需要聚类的数据映射到无向图的顶点中。基于中心连通性的聚类方法拓展图论游走次数理论，通过简单地比较相似矩阵的元素，动态智能地确定聚类中心以及数据的分类。不仅如此，我们可以通过相似矩阵不同次数的幂矩阵来适应我们想要的观察规模。

​		假设有无向图$$G$$，它的自连邻接矩阵为$$A$$。$$a{_{ij}}^{(k)}$$表示$$A$$的$$k$$次幂矩阵$$A^k$$中第$$i$$个顶点$$v_i$$到第$$j$$个顶点步长为$$k$$的数目。

​		下面举一个简单的例子。在**图1**中，在我们的三次幂邻接矩阵$$A^3$$中，$$a_{22}^{(3)}=7$$表示从$$v_2$$到$$v_2$$步长为$$3$$的数目为$$7$$，详情可见**表1**

​		然而对于真实的数据集，我们无法构造出相对应的邻接矩阵。我们只能构造数据的成对相似矩阵。因此我们有必要对上面的关于邻接矩阵的理论拓展到相似矩阵。下面的关于连接性概念只是对上面理论的一个拓展。这两个概念不同之处在于，邻接矩阵是由无向图计算出来的，邻接矩阵中所有的元素都是整数。而连接性是由数据相似矩阵所定义的，所以仅是实数就可以。

### 2.2相关概念及定义

​		**定义1(k阶连接性)**	在相似矩阵$$S$$中，$$k$$阶相似矩阵$$S^k$$中的$$s_{ij}^{(k)}$$定义为$$v_i$$和$$v_j$$的$$k$$阶连接性，用$$con^{(k)}(v_i,v_j)$$来表示。显然$$con^{(k)}(v_i,v_j)$$能够近似地代表$$v_i$$和$$v_j$$之间步长为$$k$$的数目。$$con^{(k)}(v_i,v_j)$$能够表示$$v_i$$和$$v_j$$之间的关联程度。

​		**定义2(聚类中心)**	如果顶点$$v_i$$满足以下条件，那么$$v_i$$就定义为连接中心和数据的$$k$$阶聚类中心
$$
con^{(k)}(v_i,v_i)>con^{(k)}(v_i,v_j),j=1,...,n(j\neq i)
$$
在**图1**中，经过简单计算，可以得出$$A^3$$的聚类中心为$$v_2$$

​		**定义3($$k$$阶相对连接性)**	对于任意数据点$$v_i$$和$$v_j$$，$$k$$阶相对连接性定义为
$$
rcon^{(k)}(v_i,v_j) = con^{(k)}(v_i,v_j)/con^{(k)}(v_i,v_i)
$$
$$k$$阶相对连接性相较于$$k$$阶连接性，能够消除自身连接性的影响，使得数据点分配到聚类中心的时候更加准确。

​		**定义4(无向图切图)**		 对于无向图$$G= \{ v_1,v_2,...,v_n \}$$，我们将其分割为$$m$$个子图$$A_1,A_2,...,A_m$$，并满足以下条件
$$
A_1 \cup A_2 \cup ... \cup A_m =G
$$

$$
A_i \cap A_j = \empty (i,j=1,2,...,m且i \neq j)
$$

**无向图切图权重**	对于任意两个子图$$A，B（A，B \subset G 且A \cap B = \emptyset）$$,我们定义其权重为
$$
W(A,B)=\sum_{i \in A,j \in B}w_{ij}
$$
其中$$w_{ij}$$为邻接矩阵中的元素。

那么对于$$m$$个子图，我们定义切图cut为
$$
cut(A_1,A_2,...,A_m)=\sum_{1}^{m} W(A_i, \bar{A_i})
$$
$$\bar{A_i}$$是除了$$A_i$$子图外其他子图的并集。

​		**定义6(正规化切图)**		对于定义5的切图，我们做进一步的处理
$$
Ncut(A_1,A_2,...,A_m)=\sum_{1}^{m} \frac {W(A_i,\bar{A_i})} {Vol(A_i)}
$$
其中$$Vol(A) = \sum_{i \in A} \sum_{}^{}$$



​		**分类规则**		假设现在我们有$$m$$个聚类中心$$v_{c_i}(c_i \in \{ 1,2,...,n \} 同时i=1,2,...,m)$$，对于任意数据点$$v_j$$,它将会被分配到聚类中心$$v^*$$,$$v^*$$满足由下面等式得到：
$$
v^*=\mathop{argmax}_{v_{c_i}}(rcon^{(k)}(v_{c_i},v_j))
$$
分类规则表示$$v_j$$会分类到与它相对连接性最强的聚类中心，十分的直观。



### 2.3聚类过程

​		对于每一次迭代，通过**(1)**可以得到聚类中心。通过**(2)**可以将其余的数据点分配到对应的聚类中心。

​		当$$k=1$$，时，所有的数据点都可以看作是聚类中心，初始的类别数目和数据点的数目相同。随着迭代次数$$k$$的增加，聚类中心和聚类数目反映了数据点之间的连通性。当迭代次数$$k$$继续增加趋于无穷的时候，聚类中心数目会缩减到一个点，只有一个聚类。这时候所有的数据点就属于同一类。因此我们可以将迭代次数$$k$$作为我们的观测规模。当观测规模$$k$$小的时候，数据点只会被他们邻近的数据点所影响，这时候的类别数相对较多；当观测规模$$k$$较大的时候，数据点直接的连接性会传播的更加广泛，这时候的类别数会相对较少。

​		例如在**图2**中，当$$k=1$$的时候，数据点$$v_1$$,$$v_2$$和$$v_3$$都可以看作是聚类中心，当$$k \geq 2$$时，只剩$$v_2$$这个聚类中心，其余数据点$$v_1$$和$$v_3$$都被分配到这个$$v_2$$这个聚类中心。通过**(1)**和**(2)**我们可以得到在观察规模为$$k$$时的聚类中心以及聚类结果。然而，对于一些数据集来说，他们在不同的观察规模$$k$$下有相同的类别数，但是他们的聚类中心和其余数据点分类却不完全相同。这时候我们引入正规化切图$$Ncut$$(定义6)来确定在这种情况下更好的聚类结果。正规化切图$$Ncut$$能够表示各子图之间相互关联的程度。子图数目确定的情况下，$$Ncut$$越小，各子图相互的关联性就越小。

​		显然，根据上述定义，在类别固定的情况下，对于我们所得到的多个聚类中心以及其他点的分类结果，$$Ncut$$越小，结果就更加合理。

​		

### 2.4可行性

​		我们使用基于中心连通性的聚类方法对**鸢尾属植物数据集**进行聚类

#### 2.4.1	鸢尾属植物数据集

​		鸢尾属植物数据集(Iris Data Set)是著名的数据集。在鸢尾属植物数据集中，包括了三类不同的鸢尾属植物：山鸢尾(Iris Setosa)、杂色鸢尾(Iris Versicolour)和维吉尼亚鸢尾(Iris Virginica)。此数据集中一共包含了150个样本，每个样本包含了四个特征，分别是：花萼长度(sepal length)、花萼宽度(sepel width)、花瓣长度(petal length)以及花瓣宽度(petal width)。以上四个特征的单位都是厘米(cm)。



#### 2.4.2	使用基于中心连通性聚类方法对鸢尾属植物数据集进行分类

​		对于鸢尾属植物数据集,总共有$$m=150$$个

​		

## 3.基于中心连通性聚类方法的应用

### 3.1高光谱以及band selection

3.2将CCE应用于HSI的BS

3.3Edge Detect

3.4CCE+Edge Detect+BS

