# 基于中心连通性的聚类方法分析

## 1. 绪论

​		聚类分析起源于分类学，在古老的分类学中，人们主要依靠经验和专业知识来实现分类，很少利用数学工具进行定量的分类,致使许多分类带有主观性和任意性，不能很好地揭示客观事物内在的本质区别与联系，特别是对于多个特征、多指标的分类问题更难以实现客观的分类。随着人类科学技术的发展，对分类的要求越来越高，于是人们逐渐地把数学工具引用到了分类学中，形成了数值分类学，之后又将多元分析的技术引入到数值分类学形成了聚类分析。

​		聚类分析在许多领域中都得到了广泛的应用，取得了许多成果。

### 1.1 聚类分析的基本思想

​		聚类分析认为，所研究的样本之间存在不同程度的相似性。根据多个样本的多个特征，找出能够表示样本之间的相似性的度量，并且根据这种度量采用某些聚类方法，将所有的样品分配到不同的种类当中去，使同一种类中的样本具有较大的相似性，不同类中的样本相似性小。我们将这种分类方法称为聚类分析。

### 1.2 聚类分析的内容

​		聚类中心和聚类数目的确定是聚类分析的关键。许多聚类方法已经被广泛探索过。常见的聚类方法有K-Means聚类、均值漂移聚类、基于密度的聚类、层次聚类等聚类方法、基于图论的聚类等聚类方法。本文主要介绍基于中心连通性的聚类方法。基于中心连通性的聚类方法是基于图论的方法。

​		每种聚类分析方法都涉及事物之间的相似性。聚类分析方法的本质就是寻找一个客观能反应样本之间相关联系的统计量，然后根据这种统计量把样本分成若干类，常用的统计量有距离和相似系数。

#### 1.2.1相似性度量——距离

用距离来衡量样本之间的相似程度。假设$$d(x_i,x_j)$$是样本$$x_i$$和样本$$x_j$$的距离。常用距离:

欧式距离: $$d(x_i,x_j)=[\sum_{k=1}^{p}(x_{ik}-x_{jk})^2]^\frac{1}{2}$$;

绝对距离: $$d(x_i,x_j)=\sum_{k=1}^{p}\left | x_{ik}-x_{jk} \right |$$;

Minkowski距离: $$d(x_i,x_j)=[\sum_{k=1}^{p}(x_{ik}-x_{jk})^m]^\frac{1}{m}$$



#### 1.2.2相似性度量——相关系数

对n个样本进行聚类的时候，用相似系数来衡量变量之间相关联程度。用$$c_{\alpha \beta }$$表示样本$$x_{\alpha}$$和样本$$x_{\beta}$$之间的相似系数，应当满足以下条件：

(1)	$$\left | c_{\alpha \beta } \right |\leqslant 1$$且$$c_{\alpha \alpha }= 1$$

(2)	$$c_{\alpha \beta }=_{\beta \alpha }$$

$$\left | c_{\alpha \beta } \right |$$越接近1，说明$$x_{\alpha}$$和$$x_{\beta}$$越相关，相似系数中最常用的是相关系数和夹角余弦。在基于图的聚类中，我们通常使用高斯核函数。鉴于本文研究内容，下面介绍基于中心连通性的聚类方法相关内容。



### 1.3常见的聚类算法

1.3.1 K-Means聚类算法

​		给定样本集$$D=\{ x_1,x_2,...,x_m\}$$，按照样本之间的距离大小，划分成$$k$$个簇$$C=\{C_1,C_2,...,C_k\}$$，经过$$N$$次迭代，让簇内的点紧密联系，而簇间的距离尽可能大。

​		**步骤1**	从样本集$$D$$中随机选取$$k$$个样本作为初始的$$k$$个质心向量$$\{\mu_1,\mu_2,...,\mu_k\}$$

​		**步骤2**	对于每一次迭代$$n=1,2,...,N$$,首先将簇$$C$$初始化为$$C_t=\emptyset,t=1,2,...,k$$。对于样本集$$D$$中的各样本点$$x_i(i=1,2,...,m)$$,计算他们与各质心向量$$\mu_j(j=1,2,...,k)$$的距离$$d_{ij}=\|x_i-\mu_j\|_{2}^{2}$$。将$$x_i$$标记最小的$$d_{ij}$$所对应的类别$$\lambda_i$$,更新$$C_{\lambda i}=C_{\lambda i} \cup \{x_i \}$$。对于$$j=1,2,...,k$$，对$$C_j$$中所有的样本点重新计算质心$$\mu_j=\frac {1} {\left | C_j \right |}\sum\limits_{x \in C_j}x$$,如果所有的k个质心向量都没有发生变化，则输出簇划分$$C=\{ C_1,C_2,...,C_k\}$$，否则继续迭代。

​		K-Means聚类算法的优点是速度快，计算简便，但是必须要提前将数据划分为指定的类别数，而且聚类结果受初始选择点的影响。

1.3.2DBSCAN

​		DBSCAN(具有噪声的基于密度的聚类方法)是一种基于密度的空间聚类算法。这类密度算法通过样本分布的紧密程度进行分类，即将具有足够密度的区域划分为簇，并在具有噪声的空间数据样本中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。

​		**相关定义**	首先将给定样本集$$D=\{ x_1,x_2,...,x_m\}$$通过半径$$Eps$$和样本个数阈值$$MinPts$$将数据分为三类

​		核心点：样本$$x_i$$的半径$$Eps$$的邻域内至少包含$$MinPts$$个样本，则称$$x_i$$为核心点。

​		边界点：样本$$x_i$$的半径$$Eps$$的邻域内包含的样本点数目小于$$MinPts$$，但是它在其他核心点的邻域内，则称样本$$x_i$$为边界点。

​		噪声点：样本$$x_i$$的半径$$Eps$$的邻域内包含的样本点数目小于$$MinPts$$，同时$$x_i$$也不在其他核心点的邻域内，则称样本$$x_i$$为噪声点。

​		**步骤1**	给定样本集$$D=\{ x_1,x_2,...,x_m\}$$,邻域参数($$Eps$$,$$MinPts$$),初始化核心点集合$$\Omega=\emptyset$$,初始化聚类簇数$$k=0$$,初始化未访问样本集合$$\Gamma=D$$,簇划分$$C=\emptyset$$。

​		**步骤2**	对于$$j=1,2,...,m$$，寻找核心点。根据核心点的定义，如果$$x_j$$是核心点，则将$$x_j$$加入核心点集合$$\Omega = \Omega \cup \{ x_j\}$$.

​		**步骤3**	如果核心点集合$$\Omega = \emptyset$$,则算法结束，否则进入步骤4。

​		**步骤4**	在核心点集合$$\Omega$$中，选择一个核心点$$o$$,初始化当前簇核心点集合$$\Omega _{cur}=\{ o\}$$,初始化簇类序号$$k=k+1$$,初始化当前样本集合$$C_k=\{o\}$$,更新未访问样本集合$$\Gamma = \Gamma-\{ o\}$$.

​		**步骤5**	如果当前簇核心点集合$$\Omega _{cur}=\emptyset$$,则当前聚类簇$$C_k$$生成完毕，更新簇划分$$C=\{C_1,C_2,...,C_k\}$$,更新核心点集合$$\Omega=\Omega-C_k$$,转入步骤3，否则到步骤6

​		**步骤6**	在当前簇核心点集合$$\Omega_{cur}$$中取出一个核心点$$o'$$,通过$$Eps$$找出邻域内所有样本集$$N(o')$$,令$$\Delta = N(o') \cap \Gamma$$,更新当前簇样本集合$$C_k = C_k \cup \Delta$$,更新未访问样本集合$$\Gamma = \Gamma-\Delta$$,更新$$\Omega=\Omega \cup (\Delta \cap \Omega)-o'$$,转入步骤5

​		最终输出簇划分结果$$C=\{C_1,C_2,...,C_k \}$$

​		DBSCAN的优点是不需要预先声明需要划分簇类的数目$$k$$,并且可以对任意形状稠密的数据集进行分类；在聚类的同时，可以发现异常点（噪声点），对数据集的异常点(噪声点)不敏感；不仅如此，相较于K-Means聚类算法，DBSCAN不受初始值选取的影响。

​		然而，当空间聚类的密度不均匀，参数$$Eps$$和$$MinPts$$选取困难；当数据集较大的时候，聚类收敛的时间会相对较长；相对于传统的K-Means聚类算法稍显复杂，因为需要调整参数$$Eps$$和$$MinPts$$,不同的参数组合对实际的聚类结果影响较大。



## 2.基于中心连通性的聚类方法及其研究

### 2.1基于中心连通性的聚类方法

​		基于中心连通性的聚类方法是基于图论游走次数理论的。唯一不同的是，在基于中心连通性的聚类方法当中，在无向图中的每一个顶点都是自连的。

​		我们认为聚类是一种演化的、动态的形式。一个数据点是否是聚类中心取决于我们怎样去观察它。我们将需要聚类的数据映射到无向图的顶点中。基于中心连通性的聚类方法拓展图论游走次数理论，通过简单地比较相似矩阵的元素，动态智能地确定聚类中心以及数据的分类。不仅如此，我们可以通过相似矩阵不同次数的幂矩阵来适应我们想要的观察规模。

​		假设有无向图$$G$$，它的自连邻接矩阵为$$A$$。$$a{_{ij}}^{(k)}$$表示$$A$$的$$k$$次幂矩阵$$A^k$$中第$$i$$个顶点$$v_i$$到第$$j$$个顶点步长为$$k$$的数目。

​		下面举一个简单的例子。在**图1**中，在我们的三次幂邻接矩阵$$A^3$$中，$$a_{22}^{(3)}=7$$表示从$$v_2$$到$$v_2$$步长为$$3$$的数目为$$7$$，详情可见**表1**

​		然而对于真实的数据集，我们无法构造出相对应的邻接矩阵。我们只能构造数据的成对相似矩阵。因此我们有必要对上面的关于邻接矩阵的理论拓展到相似矩阵。下面的关于连接性概念只是对上面理论的一个拓展。这两个概念不同之处在于，邻接矩阵是由无向图计算出来的，邻接矩阵中所有的元素都是整数。而连接性是由数据相似矩阵所定义的，所以仅是实数就可以。

### 2.2相关概念及定义

​		**定义1(k阶连接性)**	在相似矩阵$$S$$中，$$k$$阶相似矩阵$$S^k$$中的$$s_{ij}^{(k)}$$定义为$$v_i$$和$$v_j$$的$$k$$阶连接性，用$$con^{(k)}(v_i,v_j)$$来表示。显然$$con^{(k)}(v_i,v_j)$$能够近似地代表$$v_i$$和$$v_j$$之间步长为$$k$$的数目。$$con^{(k)}(v_i,v_j)$$能够表示$$v_i$$和$$v_j$$之间的关联程度。

​		**定义2(聚类中心)**	如果顶点$$v_i$$满足以下条件，那么$$v_i$$就定义为连接中心和数据的$$k$$阶聚类中心
$$
con^{(k)}(v_i,v_i)>con^{(k)}(v_i,v_j),j=1,...,n(j\neq i)
$$
在**图1**中，经过简单计算，可以得出$$A^3$$的聚类中心为$$v_2$$

​		**定义3($$k$$阶相对连接性)**	对于任意数据点$$v_i$$和$$v_j$$，$$k$$阶相对连接性定义为
$$
rcon^{(k)}(v_i,v_j) = con^{(k)}(v_i,v_j)/con^{(k)}(v_i,v_i)
$$
$$k$$阶相对连接性相较于$$k$$阶连接性，能够消除自身连接性的影响，使得数据点分配到聚类中心的时候更加准确。

​		**定义4(无向图切图)**		 对于无向图$$G= \{ v_1,v_2,...,v_n \}$$，我们将其分割为$$m$$个子图$$A_1,A_2,...,A_m$$，并满足以下条件
$$
A_1 \cup A_2 \cup ... \cup A_m =G
$$

$$
A_i \cap A_j = \empty (i,j=1,2,...,m且i \neq j)
$$

**无向图切图权重**	对于任意两个子图$$A，B（A，B \subset G 且A \cap B = \emptyset）$$,我们定义其权重为
$$
W(A,B)=\sum_{i \in A,j \in B}w_{ij}
$$
其中$$w_{ij}$$为邻接矩阵中的元素。

那么对于$$m$$个子图，我们定义切图cut为
$$
cut(A_1,A_2,...,A_m)=\sum_{1}^{m} W(A_i, \bar{A_i})
$$
$$\bar{A_i}$$是除了$$A_i$$子图外其他子图的并集。

​		**定义6(正规化切图)**		对于定义5的切图，我们做进一步的处理
$$
Ncut(A_1,A_2,...,A_m)=\sum_{1}^{m} \frac {W(A_i,\bar{A_i})} {Vol(A_i)}
$$
其中$$Vol(A) = \sum_{i \in A} \sum_{}^{}$$



​		**分类规则**		假设现在我们有$$m$$个聚类中心$$v_{c_i}(c_i \in \{ 1,2,...,n \} 同时i=1,2,...,m)$$，对于任意数据点$$v_j$$,它将会被分配到聚类中心$$v^*$$,$$v^*$$满足由下面等式得到：
$$
v^*=\mathop{argmax}_{v_{c_i}}(rcon^{(k)}(v_{c_i},v_j))
$$
分类规则表示$$v_j$$会分类到与它相对连接性最强的聚类中心，十分的直观。



### 2.3聚类过程

​		对于每一次迭代，通过**(1)**可以得到聚类中心。通过**(2)**可以将其余的数据点分配到对应的聚类中心。

​		当$$k=1$$，时，所有的数据点都可以看作是聚类中心，初始的类别数目和数据点的数目相同。随着迭代次数$$k$$的增加，聚类中心和聚类数目反映了数据点之间的连通性。当迭代次数$$k$$继续增加趋于无穷的时候，聚类中心数目会缩减到一个点，只有一个聚类。这时候所有的数据点就属于同一类。因此我们可以将迭代次数$$k$$作为我们的观测规模。当观测规模$$k$$小的时候，数据点只会被他们邻近的数据点所影响，这时候的类别数相对较多；当观测规模$$k$$较大的时候，数据点直接的连接性会传播的更加广泛，这时候的类别数会相对较少。

​		例如在**图2**中，当$$k=1$$的时候，数据点$$v_1$$,$$v_2$$和$$v_3$$都可以看作是聚类中心，当$$k \geq 2$$时，只剩$$v_2$$这个聚类中心，其余数据点$$v_1$$和$$v_3$$都被分配到这个$$v_2$$这个聚类中心。通过**(1)**和**(2)**我们可以得到在观察规模为$$k$$时的聚类中心以及聚类结果。然而，对于一些数据集来说，他们在不同的观察规模$$k$$下有相同的类别数，但是他们的聚类中心和其余数据点分类却不完全相同。这时候我们引入正规化切图$$Ncut$$(定义6)来确定在这种情况下更好的聚类结果。正规化切图$$Ncut$$能够表示各子图之间相互关联的程度。子图数目确定的情况下，$$Ncut$$越小，各子图相互的关联性就越小。

​		显然，根据上述定义，在类别固定的情况下，对于我们所得到的多个聚类中心以及其他点的分类结果，$$Ncut$$越小，结果就更加合理。

​		

### 2.4可行性

​		我们使用基于中心连通性的聚类方法对**鸢尾属植物数据集**进行聚类

#### 2.4.1	鸢尾属植物数据集

​		鸢尾属植物数据集(Iris Data Set)是著名的数据集。在鸢尾属植物数据集中，包括了三类不同的鸢尾属植物：山鸢尾(Iris Setosa)、杂色鸢尾(Iris Versicolour)和维吉尼亚鸢尾(Iris Virginica)。此数据集中一共包含了150个样本，每个样本包含了四个特征，分别是：花萼长度(sepal length)、花萼宽度(sepel width)、花瓣长度(petal length)以及花瓣宽度(petal width)。以上四个特征的单位都是厘米(cm)。



#### 2.4.2	使用基于中心连通性聚类方法对鸢尾属植物数据集进行分类

​		对于鸢尾属植物数据集,总共有$$m=150$$个数据，每个数据都有$$n=4$$个特征。

​		在鸢尾属植物数据集$$D=\{v_1,v_2,...,v_{150}\}$$中，$$v_k \in D$$代表鸢尾属植物数据集中第$$k$$个样本。这里我们通过高斯核函数来构造数据点之间的相似矩阵。
$$
H_{Gauss}:S_{ij}=exp(-\|v_i-v_j\|^2/\sigma^2)
$$
所得到的相似矩阵是实对称矩阵。**图3**通过**2.3节的聚类过程**，我们可以得到不同迭代次数时鸢尾属植物数据集的聚类中心和聚类数目。在**图3**中，基于中心连通性的聚类方法认为，鸢尾属植物数据集应该分为两类，而数据集本身是有三类的。在**图4**中，通过$$MDS$$方法可视化鸢尾属植物数据集，我们可以发现基于中心连通性的聚类方法将数据集分成两类是合理的。在**图4**中，我们发现，鸢尾属植物数据集的杂色鸢尾(Iris Versicolour)和维吉尼亚鸢尾(Iris Virginica)十分接近，甚至于有些混在了一起。

#### 2.4.3 调整兰德系数

​		调整兰德系数(Adjusted Rand index)用于聚类模型的性能评估。使用这个度量指标需要数据本身有标记类别。调整兰德系数是一个标量，范围在$$[-1,1]$$之间，反映了两种划分的重叠程度。它的数值越大，说明聚类的效果越好。

​		给定一个有$$n$$个样本的集合$$S=\{o_1,o_2,...,o_n\}$$,$$X=\{X_1,X_2,...,X_r\}$$和$$Y=\{Y_1,Y_2,...,Y_s\}$$是对集合$$S$$的两个不同划分。给出如下定义

​		$$a$$为在$$X$$中为同一类，在$$Y$$中也为同一类的对象对数。

​		$$b$$为在$$X$$中为同一类，在$$Y$$中不为同一类的对象对数。

​		$$c$$为在$$X$$中不为同一类，在$$Y$$中为同一类的对象对数。

​		$$d$$为在$$X$$中不为同一类，在$$Y$$中不为同一类的对象对数。

​		兰德系数(Rand Index)的计算公式为
$$
RI=\frac {a+d} {a+b+c+d}
$$
兰德系数无法保证随机划分的聚类结果的值接近0，所以提出了调整兰德系数(Adjusted Rand Index)。

调整兰德系数计算公式为
$$
ARI=\frac {RI-E[RI]} {max(RI)-E[RI]}
$$

为了计算$$ARI$$的值，我们引入列联表(contingency table),反映实际类别划分与聚类所得到的划分的重叠程度。表的行表示实际的类别划分，列表示聚类划分的簇标记,$$n_{ij}$$表示重叠实例的数量,如**表2**。

| $x/y$   |      |      |      |
| ------- | ---- | ---- | ---- |
| $$x_1$$ |      |      |      |
| $$x_2$$ |      |      |      |
| $$x_3$$ |      |      |      |
|         |      |      |      |

​		通过列联表计算$$ARI$$
$$
ARI=\frac {\sum_{ij}\binom {n_{ij}} {2}-[\sum_{i}\binom{a_i} {2}\sum_{i}\binom{b_j} {2}]/\binom{n} {2}} {\frac{1} {2}[\sum_{i}\binom{a_i} {2}+\sum_{i}\binom{b_j}{2}]-[\sum_{i}\binom{a_i} {2}\sum_{i}\binom{b_j} {2}]/\binom{n} {2}}
$$


用基于中心连通性的聚类方法将鸢尾属植物数据集分成三类所计算出的$$ARI$$的值大于用$$K-Means$$聚类方法所计算出的$$ARI$$。



## 3.基于中心连通性聚类方法的应用

### 3.1高光谱以及波段选择

#### 3.1.1高光谱遥感

​		高光谱遥感即高光谱分辨率遥感，指的是利用很多很窄的电磁波波段，从感兴趣的物体获取有关数据，将传统的空间成像与先进的光谱测量技术有机结合，具有连续光谱数据和丰富的空间信息。高光谱遥感目前已经广泛应用于各个领域。

​		高光谱遥感具有以下特点：波段多，成像光谱仪在可见光和近红外光谱区内有数十甚至数百个波段；光谱分辨率高，成像光谱仪采样的间隔小，一般为$$10nm$$左右，精细的光谱分辨率反映了地物光谱的细微特征；数据量大，随着波段数的增加，数据量呈指数增加；信息冗余，由于相邻波段的相关性高，信息冗余度增加；可提供空间域信息和光谱域信息，由成像光谱仪得到的光谱曲线可以与地面实测的同类地物光谱曲线相类比。

​		高光谱数据可以表示为高光谱数据立方体，可以看作是三维的数据图像。即在普通的二维图像以外多了一个维度的光谱信息。空间图像描述高光谱数据的地表二维空间特征，而光谱维这描述图像的光谱曲线特征，由此可以将传统的空间成像与现金的光谱测量技术有机结合。大多数地物都具有典型的光谱波形特征，尤其是光谱吸收特征。因此，从光谱数据库中将光谱匹配，可以实现地物识别的目标。

​		高光谱图像将确定物质或者地物性质的光谱与表征其空间几何特征的图像结合在一起。许多物质的特征往往表现在一些狭窄的光谱范围内，高光谱遥感实现了获取地物的光谱特征，同时又不会丢失其整体形态以及和周围地物之间关系的信息。

#### 3.1.2高光谱波段选择

​		尽管高光谱图像相较于传统的空间图像能更准确的体现地表特征和地表之间的相互联系，但也存在一些技术难点。高光谱的波段数多，数据量大，随着波段数的增加，数据量呈指数增长，容易出现维数灾难和修斯现象，使得高光谱图像的分类、识别等比较困难。由于相邻波段相关性高，导致信息冗余度高，数据存储需要花费很大的空间以及较长的时间。针对上述问题，通过降维处理高光谱数据来减少数据量和节省资源。在高光谱图像的降维处理中，特征提取和波段选择是两类常用的降维方法。

​		利用特征提取的方法对高光谱图像进行降维处理时，使用的算法比较复杂，计算量相对较大，并且是通过某种对数据的变化来达到降维的目的，改变了高光谱图像原始数据，损失了数据之间的一些相关性。

​		与高光谱图像特征提取的方法相比较，波段选择的方法更加合理。波段选择是指从高光谱图像所有的波段中选择起主要作用的波段，有效的降低了高光谱图像的维度。因为是从原来的波段集合筛选出的波段子集，所以不会改变高光谱图像的原始数据。

​		高光谱图像的波段选择是一种比较复杂的波段组合优化的问题。波段选择需要从原来的波段集中，选取出信息量大、波段间相关性小、类别之间易于区分的波段组合。

#### 3.1.3高光谱图像聚类

​		

### 3.2将CCE应用于HSI的BS

### 3.3Edge Detect

### 3.4CCE+Edge Detect+BS

