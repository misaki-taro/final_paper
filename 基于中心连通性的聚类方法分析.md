# 基于中心连通性的聚类方法分析

## 1. 绪论

​		聚类分析起源于分类学，在古老的分类学中，人们主要依靠经验和专业知识来实现分类，很少利用数学工具进行定量的分类,致使许多分类带有主观性和任意性，不能很好地揭示客观事物内在的本质区别与联系，特别是对于多个特征、多指标的分类问题更难以实现客观的分类。随着人类科学技术的发展，对分类的要求越来越高，于是人们逐渐地把数学工具引用到了分类学中，形成了数值分类学，之后又将多元分析的技术引入到数值分类学形成了聚类分析。

​		聚类分析在许多领域中都得到了广泛的应用，取得了许多成果。

### 1.1 聚类分析的基本思想

​		聚类分析认为，所研究的样本之间存在不同程度的相似性。根据多个样本的多个特征，找出能够表示样本之间的相似性的度量，并且根据这种度量采用某些聚类方法，将所有的样品分配到不同的种类当中去，使同一种类中的样本具有较大的相似性，不同类中的样本相似性小。我们将这种分类方法称为聚类分析。

### 1.2 聚类分析的内容

​		聚类中心和聚类数目的确定是聚类分析的关键。许多聚类方法已经被广泛探索过。常见的聚类方法有K-Means聚类、均值漂移聚类、基于密度的聚类、层次聚类等聚类方法、基于图论的聚类等聚类方法。本文主要介绍基于中心连通性的聚类方法。基于中心连通性的聚类方法是基于图论的方法。

​		每种聚类分析方法都涉及事物之间的相似性。聚类分析方法的本质就是寻找一个客观能反应样本之间相关联系的统计量，然后根据这种统计量把样本分成若干类，常用的统计量有距离和相似系数。

#### 1.2.1相似性度量——距离

用距离来衡量样本之间的相似程度。假设$$d(x_i,x_j)$$是样本$$x_i$$和样本$$x_j$$的距离。常用距离:

欧式距离: $$d(x_i,x_j)=[\sum_{k=1}^{p}(x_{ik}-x_{jk})^2]^\frac{1}{2}$$;

绝对距离: $$d(x_i,x_j)=\sum_{k=1}^{p}\left | x_{ik}-x_{jk} \right |$$;

Minkowski距离: $$d(x_i,x_j)=[\sum_{k=1}^{p}(x_{ik}-x_{jk})^m]^\frac{1}{m}$$



#### 1.2.2相似性度量——相关系数

对n个样本进行聚类的时候，用相似系数来衡量变量之间相关联程度。用$$c_{\alpha \beta }$$表示样本$$x_{\alpha}$$和样本$$x_{\beta}$$之间的相似系数，应当满足以下条件：

(1)	$$\left | c_{\alpha \beta } \right |\leqslant 1$$且$$c_{\alpha \alpha }= 1$$

(2)	$$c_{\alpha \beta }=_{\beta \alpha }$$

$$\left | c_{\alpha \beta } \right |$$越接近1，说明$$x_{\alpha}$$和$$x_{\beta}$$越相关，相似系数中最常用的是相关系数和夹角余弦。在基于图的聚类中，我们通常使用高斯核函数。鉴于本文研究内容，下面介绍基于中心连通性的聚类方法相关内容。

## 2.基于中心连通性的聚类方法及其研究

### 2.1基于中心连通性的聚类方法

​		基于中心连通性的聚类方法是基于图论游走次数理论的。唯一不同的是，在基于中心连通性的聚类方法当中，在无向图中的每一个顶点都是自连的。

​		我们认为聚类是一种演化的、动态的形式。一个数据点是否是聚类中心取决于我们怎样去观察它。我们将需要聚类的数据映射到无向图的顶点中。基于中心连通性的聚类方法拓展图论游走次数理论，通过简单地比较相似矩阵的元素，动态智能地确定聚类中心以及数据的分类。不仅如此，我们可以通过相似矩阵不同次数的幂矩阵来适应我们想要的观察规模。

​		假设有无向图$$G$$，它的自连邻接矩阵为$$A$$。$$a{_{ij}}^{(k)}$$表示$$A$$的$$k$$次幂矩阵$$A^k$$中第$$i$$个顶点$$v_i$$到第$$j$$个顶点步长为$$k$$的数目。

​		下面举一个简单的例子。在**图1**中，在我们的三次幂邻接矩阵$$A^3$$中，$$a_{22}^{(3)}=7$$表示从$$v_2$$到$$v_2$$步长为$$3$$的数目为$$7$$，详情可见**表1**

​		然而对于真实的数据集，我们无法构造出相对应的邻接矩阵。我们只能构造数据的成对相似矩阵。因此我们有必要对上面的关于邻接矩阵的理论拓展到相似矩阵。下面的关于连接性概念只是对上面理论的一个拓展。这两个概念不同之处在于，邻接矩阵是由无向图计算出来的，邻接矩阵中所有的元素都是整数。而连接性是由数据相似矩阵所定义的，所以仅是实数就可以。

### 2.2相关概念及定义

​		**定义1(k阶连接性)**	在相似矩阵$$S$$中，$$k$$阶相似矩阵$$S^k$$中的$$s_{ij}^{(k)}$$定义为$$v_i$$和$$v_j$$的$$k$$阶连接性，用$$con^{(k)}(v_i,v_j)$$来表示。显然$$con^{(k)}(v_i,v_j)$$能够近似地代表$$v_i$$和$$v_j$$之间步长为$$k$$的数目。$$con^{(k)}(v_i,v_j)$$能够表示$$v_i$$和$$v_j$$之间的关联程度。

​		**定义2(聚类中心)**	如果顶点$$v_i$$满足以下条件，那么$$v_i$$就定义为连接中心和数据的$$k$$阶聚类中心
$$
con^{(k)}(v_i,v_i)>con^{(k)}(v_i,v_j),j=1,...,n(j\neq i)
$$
在**图1**中，经过简单计算，可以得出$$A^3$$的聚类中心为$$v_2$$

​		**定义3($$k$$阶相对连接性)**	对于任意数据点$$v_i$$和$$v_j$$，$$k$$阶相对连接性定义为
$$
rcon^{(k)}(v_i,v_j) = con^{(k)}(v_i,v_j)/con^{(k)}(v_i,v_i)
$$
$$k$$阶相对连接性相较于$$k$$阶连接性，能够消除自身连接性的影响，使得数据点分配到聚类中心的时候更加准确。

​		**定义4(无向图切图)**		 对于无向图$$G= \{ v_1,v_2,...,v_n \}$$，我们将其分割为$$m$$个子图$$A_1,A_2,...,A_m$$，并满足以下条件
$$
A_1 \cup A_2 \cup ... \cup A_m =G
$$

$$
A_i \cap A_j = \empty (i,j=1,2,...,m且i \neq j)
$$

**无向图切图权重**	对于任意两个子图$$A，B（A，B \subset G 且A \cap B = \empty）$$,我们定义其权重为
$$
W(A,B)=\sum_{i \in A,j \in B}w_{ij}
$$
其中$$w_{ij}$$为邻接矩阵中的元素。

那么对于$$m$$个子图，我们定义切图cut为
$$
cut(A_1,A_2,...,A_m)=\sum_{1}^{m} W(A_i, \bar{A_i})
$$
$$\bar{A_i}$$是除了$$A_i$$子图外其他子图的并集。

​		**定义6(正规化切图)**		对于定义5的切图，我们做进一步的处理
$$
Ncut(A_1,A_2,...,A_m)=\sum_{1}^{m} \frac {W(A_i,\bar{A_i})} {Vol(A_i)}
$$
其中$$Vol(A) = \sum_{i \in A} \sum_{}^{}$$



​		**分类规则**		假设现在我们有$$m$$个聚类中心$$v_{c_i}(c_i \in \{ 1,2,...,n \} 同时i=1,2,...,m)$$，对于任意数据点$$v_j$$,它将会被分配到聚类中心$$v^*$$,$$v^*$$满足由下面等式得到：
$$
v^*=\mathop{argmax}_{v_{c_i}}(rcon^{(k)}(v_{c_i},v_j))
$$
分类规则表示$$v_j$$会分类到与它相对连接性最强的聚类中心，十分的直观。



### 2.3聚类过程

​		对于每一次迭代，通过**(1)**可以得到聚类中心。通过**(2)**可以将其余的数据点分配到对应的聚类中心。

​		当$$k=1$$，时，所有的数据点都可以看作是聚类中心，初始的类别数目和数据点的数目相同。随着迭代次数$$k$$的增加，聚类中心和聚类数目反映了数据点之间的连通性。当迭代次数$$k$$继续增加趋于无穷的时候，聚类中心数目会缩减到一个点，只有一个聚类。这时候所有的数据点就属于同一类。因此我们可以将迭代次数$$k$$作为我们的观测规模。当观测规模$$k$$小的时候，数据点只会被他们邻近的数据点所影响，这时候的类别数相对较多；当观测规模$$k$$较大的时候，数据点直接的连接性会传播的更加广泛，这时候的类别数会相对较少。

​		例如在**图2**中，当$$k=1$$的时候，数据点$$v_1$$,$$v_2$$和$$v_3$$都可以看作是聚类中心，当$$k \geq 2$$时，只剩$$v_2$$这个聚类中心，其余数据点$$v_1$$和$$v_3$$都被分配到这个$$v_2$$这个聚类中心。通过**(1)**和**(2)**我们可以得到在观察规模为$$k$$时的聚类中心以及聚类结果。然而，对于一些数据集来说，他们在不同的观察规模$$k$$下有相同的类别数，但是他们的聚类中心和其余数据点分类却不完全相同。这时候我们引入正规化图切$$Ncut$$来确定在这种情况下更好的聚类结果。

### 2.4可行性（iris，ARI？）

## 3.基于中心连通性聚类方法的应用

### 3.1高光谱以及band selection

3.2将CCE应用于HSI的BS

3.3Edge Detect

3.4CCE+Edge Detect+BS